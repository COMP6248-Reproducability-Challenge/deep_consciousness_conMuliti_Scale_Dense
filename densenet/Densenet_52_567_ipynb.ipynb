{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“Densenet_52_56.ipynb”的副本",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmMLg7R548uu",
        "colab_type": "code",
        "outputId": "f924af14-2cc4-4d3f-cc56-da88f07c8c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "# Execute this code block to install dependencies when running on colab\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    from os.path import exists\n",
        "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "try: \n",
        "    import torchbearer\n",
        "except:\n",
        "    !pip install torchbearer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchbearer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/62/79c45d98e22e87b44c9b354d1b050526de80ac8a4da777126b7c86c2bb3e/torchbearer-0.3.0.tar.gz (84kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 32.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4 in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from torchbearer) (0.2.2.post3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchbearer) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4->torchbearer) (1.16.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbearer) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbearer) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->torchbearer) (0.46)\n",
            "Building wheels for collected packages: torchbearer\n",
            "  Building wheel for torchbearer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/cb/69/466aef9cee879fb8f645bd602e34d45e754fb3dee2cb1a877a\n",
            "Successfully built torchbearer\n",
            "Installing collected packages: torchbearer\n",
            "Successfully installed torchbearer-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrMHjFHPgLFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def write_csv(file_path,text):\n",
        "    with open(file_path, mode='a',encoding='utf8',newline ='') as f:\n",
        "        f = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "        f.writerow(text)\n",
        "flops_file=\"resnet_flops.csv\"\n",
        "acc_file=\"resnet_acc.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXknhsKl5kwO",
        "colab_type": "code",
        "outputId": "4fe8b074-1b53-421b-b71c-04445c320aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "train_set = datasets.CIFAR10('../data', train=True, download=True,\n",
        "                              transform=transforms.Compose([\n",
        "                                  transforms.RandomCrop(32, padding=4),\n",
        "                                  transforms.RandomHorizontalFlip(),\n",
        "                                  transforms.ToTensor(),\n",
        "                              ]))\n",
        "val_set = datasets.CIFAR10('../data', train=False,\n",
        "                            transform=transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                            ]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 170172416/170498071 [00:29<00:00, 5506552.57it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kWAVLOhBowG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_set,\n",
        "        batch_size=64, shuffle=True,\n",
        "        num_workers=0, pin_memory=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_set,\n",
        "        batch_size=64, shuffle=False,\n",
        "        num_workers=0, pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yk8NN67x1FO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate FLOPs\n",
        "from torch.autograd import Variable\n",
        "from functools import reduce\n",
        "import operator\n",
        "\n",
        "count_ops = 0\n",
        "count_params = 0\n",
        "module_number = 0\n",
        "modules_flops = []\n",
        "modules_params = []\n",
        "to_print = False\n",
        "\n",
        "def get_num_gen(gen):\n",
        "    return sum(1 for x in gen)\n",
        "\n",
        "\n",
        "def is_pruned(layer):\n",
        "    try:\n",
        "        layer.mask\n",
        "        return True\n",
        "    except AttributeError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def is_leaf(model):\n",
        "    return get_num_gen(model.children()) == 0\n",
        "\n",
        "\n",
        "def get_layer_info(layer):\n",
        "    layer_str = str(layer)\n",
        "    type_name = layer_str[:layer_str.find('(')].strip()\n",
        "    return type_name\n",
        "  \n",
        "def get_layer_param(model):\n",
        "    return sum([reduce(operator.mul, i.size(), 1) for i in model.parameters()])\n",
        "\n",
        "def measure_layer(layer, x):\n",
        "    global count_ops, count_params, module_number, modules_flops\n",
        "    global modules_params, to_print\n",
        "    delta_ops = 0\n",
        "    delta_params = 0\n",
        "    multi_add = 1\n",
        "    if to_print:\n",
        "        print(\"\")\n",
        "\n",
        "    type_name = get_layer_info(layer)\n",
        "\n",
        "    ### ops_conv\n",
        "    if type_name in ['Conv2d']:\n",
        "        out_h = int((x.size()[2] + 2 * layer.padding[0] - layer.kernel_size[0]) /\n",
        "                    layer.stride[0] + 1)\n",
        "        out_w = int((x.size()[3] + 2 * layer.padding[1] - layer.kernel_size[1]) /\n",
        "                    layer.stride[1] + 1)\n",
        "        delta_ops = layer.in_channels * layer.out_channels * layer.kernel_size[0] *  \\\n",
        "                layer.kernel_size[1] * out_h * out_w / layer.groups * multi_add\n",
        "        delta_params = get_layer_param(layer)\n",
        "        if hasattr(layer, 'shared'):\n",
        "            delta_params = delta_params / int(layer.shared)\n",
        "        module_number += 1\n",
        "        modules_flops.append(delta_ops)\n",
        "        modules_params.append(delta_params)\n",
        "        if to_print:\n",
        "            print(layer)\n",
        "            print(\"Module number: \", module_number)\n",
        "            print(\"FLOPS:\", delta_ops)\n",
        "            print(\"Parameter:\", delta_params)\n",
        "\n",
        "    ### ops_nonlinearity\n",
        "    elif type_name in ['ReLU']:\n",
        "        delta_ops = x.numel()\n",
        "        delta_params = get_layer_param(layer)\n",
        "        # module_number += 1\n",
        "        # modules_flops.append(delta_ops)\n",
        "        # to_print:\n",
        "        #   print(layer)\n",
        "        #   print(\"Module number: \", module_number)\n",
        "        #   print(\"FLOPS:\", delta_ops)\n",
        "\n",
        "\n",
        "    ### ops_pooling\n",
        "    elif type_name in ['AvgPool2d']:\n",
        "        in_w = x.size()[2]\n",
        "        kernel_ops = layer.kernel_size * layer.kernel_size\n",
        "        out_w = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n",
        "        out_h = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n",
        "        delta_ops = x.size()[0] * x.size()[1] * out_w * out_h * kernel_ops\n",
        "        delta_params = get_layer_param(layer)\n",
        "\n",
        "    elif type_name in ['AdaptiveAvgPool2d']:\n",
        "        delta_ops = x.size()[0] * x.size()[1] * x.size()[2] * x.size()[3]\n",
        "        delta_params = get_layer_param(layer)\n",
        "\n",
        "    ### ops_linear\n",
        "    elif type_name in ['Linear']:\n",
        "        weight_ops = layer.weight.numel() * multi_add\n",
        "        bias_ops = layer.bias.numel()\n",
        "        delta_ops = x.size()[0] * (weight_ops + bias_ops)\n",
        "        delta_params = get_layer_param(layer)\n",
        "        # module_number += 1\n",
        "        # modules_flops.append(delta_ops)\n",
        "        # if to_print:\n",
        "        #   print(\"Module number: \", module_number)\n",
        "        #   print(\"FLOPS:\", delta_ops)\n",
        "        #   print(\"##Current params: \", count_params)\n",
        "\n",
        "    ### ops_nothing\n",
        "    elif type_name in ['BatchNorm2d', 'Dropout2d', 'DropChannel', 'Dropout']:\n",
        "        delta_params = get_layer_param(layer)\n",
        "\n",
        "    ### unknown layer type\n",
        "    else:\n",
        "        raise TypeError('unknown layer type: %s' % type_name)\n",
        "\n",
        "    count_ops += delta_ops\n",
        "    count_params += delta_params\n",
        "    layer.flops = delta_ops\n",
        "    layer.params = delta_params\n",
        "    return\n",
        "  \n",
        "def measure_model(model, H, W, debug=False):\n",
        "    global count_ops, count_params, module_number, modules_flops\n",
        "    global modules_params, to_print\n",
        "    count_ops = 0\n",
        "    count_params = 0\n",
        "    module_number = 0\n",
        "    modules_flops = []\n",
        "    modules_params = []\n",
        "    to_print = debug\n",
        "\n",
        "    data = Variable(torch.zeros(1, 3, H, W))\n",
        "\n",
        "    def should_measure(x):\n",
        "        return is_leaf(x) or is_pruned(x)\n",
        "\n",
        "    def modify_forward(model):\n",
        "        for child in model.children():\n",
        "            if should_measure(child):\n",
        "                def new_forward(m):\n",
        "                    def lambda_forward(x):\n",
        "                        measure_layer(m, x)\n",
        "                        return m.old_forward(x)\n",
        "                    return lambda_forward\n",
        "                child.old_forward = child.forward\n",
        "                child.forward = new_forward(child)\n",
        "            else:\n",
        "                modify_forward(child)\n",
        "\n",
        "    def restore_forward(model):\n",
        "        for child in model.children():\n",
        "            # leaf node\n",
        "            if is_leaf(child) and hasattr(child, 'old_forward'):\n",
        "                child.forward = child.old_forward\n",
        "                child.old_forward = None\n",
        "            else:\n",
        "                restore_forward(child)\n",
        "\n",
        "    modify_forward(model)\n",
        "    model.forward(data)\n",
        "    restore_forward(model)\n",
        "\n",
        "    if to_print:\n",
        "        print(\"modules flops sum: \", sum(modules_flops[0:2]))\n",
        "    return count_ops, count_params\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYj7w8GSEKH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(self.relu(self.bn1(x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        return torch.cat([x, out], 1)\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, dropRate=0.0):\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        inter_planes = out_planes * 4\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, inter_planes, kernel_size=1, stride=1,\n",
        "                               padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(inter_planes)\n",
        "        self.conv2 = nn.Conv2d(inter_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(self.relu(self.bn1(x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n",
        "        out = self.conv2(self.relu(self.bn2(out)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n",
        "        return torch.cat([x, out], 1)\n",
        "\n",
        "class TransitionBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, dropRate=0.0):\n",
        "        super(TransitionBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
        "                               padding=0, bias=False)\n",
        "        self.droprate = dropRate\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(self.relu(self.bn1(x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n",
        "        return F.avg_pool2d(out, 2)\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, growth_rate, block, dropRate=0.0):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, growth_rate, nb_layers, dropRate)\n",
        "    def _make_layer(self, block, in_planes, growth_rate, nb_layers, dropRate):\n",
        "        layers = []\n",
        "        for i in range(nb_layers):\n",
        "            layers.append(block(in_planes+i*growth_rate, growth_rate, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class DenseNet3(nn.Module):\n",
        "    def __init__(self, depth, num_classes, growth_rate=12,\n",
        "                 reduction=0.5, bottleneck=True, dropRate=0.0):\n",
        "        super(DenseNet3, self).__init__()\n",
        "        in_planes = 2 * growth_rate\n",
        "        n = (depth - 4) / 3\n",
        "        if bottleneck == True:\n",
        "            n = n/2\n",
        "            block = BottleneckBlock\n",
        "        else:\n",
        "            block = BasicBlock\n",
        "        n = int(n)\n",
        "        # 1st conv before any dense block\n",
        "        self.conv1 = nn.Conv2d(3, in_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = DenseBlock(n \n",
        "                                 , in_planes, growth_rate, block, dropRate)\n",
        "        in_planes = int(in_planes+n*growth_rate)\n",
        "        self.trans1 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)), dropRate=dropRate)\n",
        "        in_planes = int(math.floor(in_planes*reduction))\n",
        "        # 2nd block\n",
        "        self.block2 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n",
        "        in_planes = int(in_planes+n*growth_rate)\n",
        "        self.trans2 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)), dropRate=dropRate)\n",
        "        in_planes = int(math.floor(in_planes*reduction))\n",
        "        # 3rd block\n",
        "        self.block3 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n",
        "        in_planes = int(in_planes+n*growth_rate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(in_planes, num_classes)\n",
        "        self.in_planes = in_planes\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.block1(out))\n",
        "        out = self.trans2(self.block2(out))\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.in_planes)\n",
        "        return self.fc(out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9osVY256hy8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "917e096e-a4c0-4224-c9d8-a9e1ba45ce6e"
      },
      "source": [
        "model=DenseNet3(52,10)\n",
        "n_flops, n_params = measure_model(model, 32,32)\n",
        "print('Finished training! FLOPs: %.2fM, Params: %.2fM' % (n_flops / 1e6, n_params / 1e6))\n",
        "write_csv(flops_file,[1,str(n_flops)])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished training! FLOPs: 107.32M, Params: 0.26M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovlZNTU_Inkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchbearer\n",
        "from torchbearer import Trial\n",
        "from torch import optim\n",
        "model=DenseNet3(52,10)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(model.parameters())\n",
        "\n",
        "#print(model)\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy']).to(device)\n",
        "trial.with_generators(train_loader, val_generator=val_loader)\n",
        "trial.run(epochs=30)\n",
        "results = trial.evaluate(data_key=torchbearer.VALIDATION_DATA)\n",
        "print()\n",
        "print(results)\n",
        "write_csv(acc_file,[1,str(results['val_acc'])])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhuFF7nddtQy",
        "colab_type": "code",
        "outputId": "6c30a834-debb-4f17-d043-ba945e404c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished training! FLOPs: 107.32M, Params: 0.26M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blRXy-FOWd5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseNet6(nn.Module):\n",
        "    def __init__(self, depth, num_classes, growth_rate=12,\n",
        "                 reduction=0.5, bottleneck=True, dropRate=0.0):\n",
        "        super(DenseNet6, self).__init__()\n",
        "        in_planes = 2 * growth_rate\n",
        "        n = (depth - 4) / 3\n",
        "        if bottleneck == True:\n",
        "            n = n/2\n",
        "            block = BottleneckBlock\n",
        "        else:\n",
        "            block = BasicBlock\n",
        "        n = int(n)\n",
        "        # 1st conv before any dense block\n",
        "        self.conv1 = nn.Conv2d(3, in_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n",
        "        in_planes = int(in_planes+n*growth_rate)\n",
        "        self.trans1 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)), dropRate=dropRate)\n",
        "        in_planes = int(math.floor(in_planes*reduction))\n",
        "        # 2nd block\n",
        "        self.block2 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n",
        "        in_planes = int(in_planes+n*growth_rate)\n",
        "        self.trans2 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)), dropRate=dropRate)\n",
        "        in_planes = int(math.floor(in_planes*reduction))\n",
        "        # 3rd block\n",
        "        self.block3 = DenseBlock(6, in_planes, growth_rate, block, dropRate)\n",
        "        in_planes = int(in_planes+6*growth_rate)\n",
        "#         # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(in_planes, num_classes)\n",
        "        self.in_planes = in_planes\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.block1(out))\n",
        "        out = self.trans2(self.block2(out))\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, out.size(2))\n",
        "        out = out.view(-1, self.in_planes)\n",
        "        return self.fc(out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwChQ8smXphg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=DenseNet6(52,10)\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaY_M0OtZ5iw",
        "colab_type": "code",
        "outputId": "f5d65cdf-6171-4034-c81b-b14daaa20132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "n_flops, n_params = measure_model(model, 32,32)\n",
        "print('Finished training! FLOPs: %.2fM, Params: %.2fM' % (n_flops / 1e6, n_params / 1e6))\n",
        "write_csv(flops_file,[1,str(n_flops)])\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished training! FLOPs: 105.67M, Params: 0.24M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ji2PFF2XY70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=64, shuffle=True,\n",
        "    num_workers=0, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set,\n",
        "    batch_size=64, shuffle=False,\n",
        "    num_workers=0, pin_memory=True)\n",
        "\n",
        "\n",
        "import torchbearer\n",
        "from torchbearer import Trial\n",
        "from torch import optim\n",
        "model=DenseNet6(52,10)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(model.parameters())\n",
        "\n",
        "#print(model)\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy']).to(device)\n",
        "trial.with_generators(train_loader, val_generator=val_loader)\n",
        "trial.run(epochs=30)\n",
        "results = trial.evaluate(data_key=torchbearer.VALIDATION_DATA)\n",
        "print()\n",
        "print(results)\n",
        "write_csv(acc_file,[1,str(results['val_acc'])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akM9YCw2dEfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for param in model.parameters():\n",
        "#     param.requires_grad=True\n",
        "\n",
        "# loss_function = nn.CrossEntropyLoss()\n",
        "# optimiser = optim.Adam(model.parameters())\n",
        "\n",
        "# #print(model)\n",
        "\n",
        "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy']).to(device)\n",
        "# trial.with_generators(train_loader, val_generator=val_loader)\n",
        "# trial.run(epochs=10)\n",
        "# results = trial.evaluate(data_key=torchbearer.VALIDATION_DATA)\n",
        "# print()\n",
        "# print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV9hXGah0l58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  block3_6th layer output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4JOfL000m7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseNet5(nn.Module):\n",
        "    def __init__(self, depth, num_classes, growth_rate=12,\n",
        "                 reduction=0.5, bottleneck=True, dropRate=0.0):\n",
        "        super(DenseNet5, self).__init__()\n",
        "        in_planes = 2 * growth_rate\n",
        "        n = (depth - 4) / 3\n",
        "        if bottleneck == True:\n",
        "            n = n/2\n",
        "            block = BottleneckBlock\n",
        "        else:\n",
        "            block = BasicBlock\n",
        "        n = int(n)\n",
        "        # 1st conv before any dense block\n",
        "        self.conv1 = nn.Conv2d(3, in_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n",
        "        in_planes = int(in_planes+n*growth_rate)\n",
        "        self.trans1 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)), dropRate=dropRate)\n",
        "        in_planes = int(math.floor(in_planes*reduction))\n",
        "        # 2nd block\n",
        "        self.block2 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n",
        "        in_planes = int(in_planes+n*growth_rate)\n",
        "        self.trans2 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)), dropRate=dropRate)\n",
        "        in_planes = int(math.floor(in_planes*reduction))\n",
        "        # 3rd block\n",
        "        self.block3 = DenseBlock(3, in_planes, growth_rate, block, dropRate)\n",
        "        in_planes = int(in_planes+3*growth_rate)\n",
        "#         # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(in_planes, num_classes)\n",
        "        self.in_planes = in_planes\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.block1(out))\n",
        "        out = self.trans2(self.block2(out))\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, out.size(2))\n",
        "        out = out.view(-1, self.in_planes)\n",
        "        return self.fc(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4GEojymyyUR",
        "colab_type": "code",
        "outputId": "2fb955ad-df2c-459e-dc57-406a9b82817e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model=DenseNet5(52,10)\n",
        "#print(model)\n",
        "n_flops, n_params = measure_model(model, 32,32)\n",
        "print('Finished training! FLOPs: %.2fM, Params: %.2fM' % (n_flops / 1e6, n_params / 1e6))\n",
        "write_csv(flops_file,[1,str(n_flops)])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished training! FLOPs: 103.48M, Params: 0.20M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcpG4IrLx3RH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ceng\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=64, shuffle=True,\n",
        "    num_workers=0, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set,\n",
        "    batch_size=64, shuffle=False,\n",
        "    num_workers=0, pin_memory=True)\n",
        "\n",
        "\n",
        "import torchbearer\n",
        "from torchbearer import Trial\n",
        "from torch import optim\n",
        "model=DenseNet6(52,10)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(model.parameters())\n",
        "\n",
        "#print(model)\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy']).to(device)\n",
        "trial.with_generators(train_loader, val_generator=val_loader)\n",
        "trial.run(epochs=30)\n",
        "results = trial.evaluate(data_key=torchbearer.VALIDATION_DATA)\n",
        "print()\n",
        "print(results)\n",
        "write_csv(acc_file,[1,str(results['val_acc'])])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}